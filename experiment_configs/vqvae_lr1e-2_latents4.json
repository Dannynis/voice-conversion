{
    "seed": 154367,
    "experiment_name": "vqvae_lr1e-2_latents4",
    "batch_size": 24,
    "num_epochs": 200,
    "input_max_len": 20000,
    "commit_coefficient": 0.25,
    "comment_commit_coefficient": "Should be in the range 0.1-2.0.",
    "learning_rate": 1e-2,
    "weight_decay_coefficient": 1e-6,
    "num_input_quantization_channels": 256,
    "encoder": {
        "kernel_sizes": [7, 7, 7, 7],
        "dilations": [1, 2, 4, 16],
        "strides": [3, 3, 3, 3],
        "num_residual_channels": [16, 32, 64, 64]
    },
    "vq": {
        "latent_dim": 64,
        "num_latent": 4
    },
    "generator": {
        "kernel_sizes": [7, 7, 7, 7, 100],
        "strides": [3, 3, 3, 3, 1],
        "comment_strides": "TODO: Check receptive field, maybe use dilations?",
        "dilations": [1, 2, 4, 16, 1],
        "comment_dilations": "effective kernel size = kernel + (kernel - 1)*(dilation -1)",
        "paddings": [3, 2, 3, 2, 1],
        "out_paddings": [0, 0, 0, 1, 0],
        "comment_out_paddings": "output = (input - 1)*stride - 2*padding + kernel + output_padding",
        "num_residual_channels": [64, 64, 32, 32, 32],
        "pre_output_channels": 64,
        "comment_pre_output_channels": "TODO: decide on all the channel dimensionalities.",
        "num_final_output_channels": 256,
        "speaker_dim": 64
    }
}
